{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGyrK90IJMtzdsZAHJDh/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/get_nbm_aws_streamline_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mCU01BOdhCx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "!pip install pygrib\n",
        "\n",
        "import os, gc\n",
        "import boto3\n",
        "import pygrib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "\n",
        "from functools import partial\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "from datetime import datetime, timedelta\n",
        "from multiprocessing import cpu_count, Pool"
      ],
      "metadata": {
        "id": "ZP6Vq6urQJst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "RgfkSfCGhEe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiprocess settings\n",
        "process_pool_size = cpu_count()*8\n",
        "print(f'Process Pool Size: {process_pool_size}')\n",
        "\n",
        "# Define Globals\n",
        "aws_bucket = 'noaa-nbm-grib2-pds'\n",
        "\n",
        "# Where to place the grib file (subdirs can be added in local) (not used)\n",
        "# output_dir = './'\n",
        "\n",
        "# Which grib variables do each element correlate with\n",
        "element_var = {'qpf':'APCP',\n",
        "                  'maxt':'TMP',\n",
        "                  'mint':'TMP'}\n",
        "\n",
        "# Which grib levels do each element correlate with\n",
        "element_lev = {'qpf':'surface',\n",
        "               'maxt':'2 m above ground',\n",
        "               'mint':'2 m above ground'}\n",
        "\n",
        "# If a grib message contains any of these, exclude\n",
        "excludes = ['ens std dev', '% lev']"
      ],
      "metadata": {
        "id": "mlF-5hehQNbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "T7TMDADMhK5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro8LqKJLQB4r"
      },
      "outputs": [],
      "source": [
        "def fetch_grib_from_AWS(iter_item, **req):\n",
        "\n",
        "    yyyymmdd = iter_item\n",
        "\n",
        "    nbm_sets = ['core', 'qmd'] if ((element == 'qpf') &\n",
        "                (req['hh'] % 6 == 0)) else ['core']\n",
        "\n",
        "    output_file = f'{yyyymmdd}.t{req[\"hh\"]:02d}z.fhr{req[\"fhr\"]:03d}.{req[\"var\"]}.grib2'\n",
        "\n",
        "    if os.path.isfile(output_file):\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        for nbm_set in nbm_sets:\n",
        "\n",
        "            bucket_dir = f'blend.{yyyymmdd}/{req[\"hh\"]:02d}/{nbm_set}/'\n",
        "\n",
        "            grib_file = f'{bucket_dir}blend.t{req[\"hh\"]:02d}z.'+\\\n",
        "                        f'{nbm_set}.f{req[\"fhr\"]:03d}.{req[\"nbm_area\"]}.grib2'\n",
        "\n",
        "            index_file = f'{grib_file}.idx'\n",
        "\n",
        "            client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "            index_data_raw = client.get_object(\n",
        "                Bucket=aws_bucket, Key=index_file)['Body'].read().decode().split('\\n')\n",
        "\n",
        "            cols = ['num', 'byte', 'date', 'var', 'level',\n",
        "                'forecast', 'fthresh', 'ftype', '']\n",
        "\n",
        "            index_data = pd.DataFrame([item.split(':') for item in index_data_raw],\n",
        "                            columns=cols if nbm_set == 'core' else cols[:-1])\n",
        "\n",
        "            # Clean up any ghost indicies, set the index\n",
        "            index_data = index_data[index_data['num'] != '']\n",
        "            index_data['num'] = index_data['num'].astype(int)\n",
        "            index_data = index_data.set_index('num')\n",
        "\n",
        "            # Allow byte ranging to '' (EOF)\n",
        "            index_data.loc[index_data.shape[0]+1] = ['']*index_data.shape[1]\n",
        "\n",
        "            index_subset = index_data[\n",
        "                ((index_data['var'] == req['var']) &\n",
        "                (index_data['level'] == req['level']))]\n",
        "\n",
        "            # byte start >> byte range\n",
        "            for i in index_subset.index:\n",
        "                index_subset.loc[i]['byte'] = (\n",
        "                    index_data.loc[i, 'byte'],\n",
        "                    index_data.loc[int(i)+1, 'byte'])\n",
        "\n",
        "            # Filter out excluded vars\n",
        "            for ex in excludes:\n",
        "                mask = np.column_stack([index_subset[col].str.contains(ex, na=False)\n",
        "                                        for col in index_subset])\n",
        "\n",
        "                index_subset = index_subset.loc[~mask.any(axis=1)]\n",
        "\n",
        "            # Fetch the data by byte range, write from stream\n",
        "            for index, item in index_subset.iterrows():\n",
        "                byte_range = f\"bytes={item['byte'][0]}-{item['byte'][1]}\"\n",
        "\n",
        "                output_bytes = client.get_object(\n",
        "                    Bucket=aws_bucket, Key=grib_file, Range=byte_range)\n",
        "\n",
        "                with open(output_file, 'ab') as wfp:\n",
        "                    for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                        wfp.write(chunk)\n",
        "\n",
        "    client.close()\n",
        "    return output_file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grib2xarray(grib_file_path, precip_interval=None):\n",
        "\n",
        "    # Create a list to store the data arrays for each variable\n",
        "    data_arrays = []\n",
        "\n",
        "    # Open the GRIB2 file using pygrib\n",
        "    with pygrib.open(grib_file_path) as grib_file:\n",
        "        print('\\nreading: ', grib_file_path)\n",
        "\n",
        "        # Iterate over each message in the GRIB2 file\n",
        "        for msg in grib_file:\n",
        "\n",
        "            if (('Probability' in str(msg))\n",
        "                & (msg['lengthOfTimeRange'] == precip_interval)):\n",
        "\n",
        "                threshold_in = round(msg['upperLimit']*0.0393701, 2)\n",
        "\n",
        "                if threshold_in <= 4.0:\n",
        "\n",
        "                    valid_time = datetime.strptime(\n",
        "                        f\"{msg['validityDate']}{msg['validityTime']:02d}\",\n",
        "                        '%Y%m%d%H')\n",
        "\n",
        "                    # Extract data and metadata from the GRIB2 message\n",
        "                    data = msg.values\n",
        "                    lats, lons = msg.latlons()\n",
        "\n",
        "                    # Create an xarray DataArray for the variable\n",
        "                    da = xr.DataArray(data,\n",
        "                                    coords={'lat': lats[:, 0],\n",
        "                                            'lon': lons[0, :]},\n",
        "                                    dims=['lat', 'lon'])\n",
        "\n",
        "                    # Add variable metadata as attributes (slow, not needed?)\n",
        "                    # for key in msg.keys():\n",
        "                    #     if key not in ['values', 'latlons']:\n",
        "                    #         try:\n",
        "                    #             da.attrs[key] = msg[key]\n",
        "                    #         except:\n",
        "                    #             pass\n",
        "\n",
        "                    da.name = f\"tp_gt_{str(threshold_in).replace('.','p')}\"\n",
        "                    da['valid_time'] = valid_time\n",
        "\n",
        "                    # Add the DataArray to the list\n",
        "                    data_arrays.append(da)\n",
        "\n",
        "    # Combine the list of DataArrays into a single xarray dataset\n",
        "    ds = xr.merge(data_arrays, compat='override')\n",
        "    gc.collect()\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "aMbqBuI8ETMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Input/Multiprocessing Inputs"
      ],
      "metadata": {
        "id": "AuVwvcXWhMPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "element = 'qpf' #input('Desired element? (QPF/MaxT/MinT)').lower()\n",
        "\n",
        "start_date = '20231001'\n",
        "end_date = '20231031'\n",
        "\n",
        "# Immediately convert user input to datetime objects\n",
        "start_date, end_date = [datetime.strptime(date+'0000', '%Y%m%d%H%M')\n",
        "    for date in [start_date, end_date]]"
      ],
      "metadata": {
        "id": "vtgrpivgQTNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main/Multiprocessing Call"
      ],
      "metadata": {
        "id": "Irb9HzFAhTZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build arg dict\n",
        "nbm_request_args = {\n",
        "    #'yyyymmdd':yyyymmdd, #input('Desired init date (YYYYMMDD)? '),\n",
        "    'hh':00, #int(input('Desired init hour int(HH)? ')),\n",
        "    'fhr':24, #int(input('Desired forecast hour/lead time int(HHH)?')),\n",
        "    'nbm_area':'co',\n",
        "    'var':element_var[element],\n",
        "    'level':element_lev[element]}"
      ],
      "metadata": {
        "id": "igAjCkwNhYYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= end_date:\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_grib_from_AWS, **nbm_request_args)\n",
        "\n",
        "# Set up this way for later additions (e.g. a 2D iterable)\n",
        "# multiprocess_iterable = [item for item in itertools.product(\n",
        "#     other_iterable, date_selection_iterable)]\n",
        "multiprocess_iterable = date_selection_iterable\n",
        "\n",
        "with Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} tasks '\n",
        "          f'across {process_pool_size} workers')\n",
        "    output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "    print('Multiprocessing Complete')"
      ],
      "metadata": {
        "id": "1-ReDYLChqO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_grib2xarray = partial(grib2xarray, precip_interval=24)\n",
        "\n",
        "with Pool(8) as pool:\n",
        "    ds_list = pool.map(mp_grib2xarray, output_files[:4])\n",
        "    pool.terminate()\n",
        "\n",
        "# Compile along time axis\n",
        "ds = xr.concat(ds_list, dim='valid_time')"
      ],
      "metadata": {
        "id": "dzlEEP0w9-iJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}