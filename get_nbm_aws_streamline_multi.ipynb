{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU3oxsXcqJUa82FBsbFPWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/get_nbm_aws_streamline_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mCU01BOdhCx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "!pip install pygrib\n",
        "\n",
        "import os, boto3, pygrib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "from datetime import datetime, timedelta\n",
        "from multiprocessing import cpu_count, Pool"
      ],
      "metadata": {
        "id": "ZP6Vq6urQJst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "RgfkSfCGhEe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiprocess settings\n",
        "process_pool_size = cpu_count()*8\n",
        "print(f'Process Pool Size: {process_pool_size}')\n",
        "\n",
        "# Define Globals\n",
        "aws_bucket = 'noaa-nbm-grib2-pds'\n",
        "\n",
        "# Where to place the grib file (subdirs can be added in local) (not used)\n",
        "# output_dir = './'\n",
        "\n",
        "# Which grib variables do each element correlate with\n",
        "element_var = {'qpf':'APCP',\n",
        "                  'maxt':'TMP',\n",
        "                  'mint':'TMP'}\n",
        "\n",
        "# Which grib levels do each element correlate with\n",
        "element_lev = {'qpf':'surface',\n",
        "               'maxt':'2 m above ground',\n",
        "               'mint':'2 m above ground'}\n",
        "\n",
        "# If a grib message contains any of these, exclude\n",
        "excludes = ['ens std dev', '% lev']"
      ],
      "metadata": {
        "id": "mlF-5hehQNbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "T7TMDADMhK5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro8LqKJLQB4r"
      },
      "outputs": [],
      "source": [
        "def fetch_grib_from_AWS(iter_item, **req):\n",
        "\n",
        "    yyyymmdd = iter_item\n",
        "\n",
        "    nbm_sets = ['core', 'qmd'] if ((element == 'qpf') &\n",
        "                (req['hh'] % 6 == 0)) else ['core']\n",
        "\n",
        "    output_file = f'{yyyymmdd}.t{req[\"hh\"]:02d}z.fhr{req[\"fhr\"]:03d}.{req[\"var\"]}.grib2'\n",
        "\n",
        "    if os.path.isfile(output_file):\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        for nbm_set in nbm_sets:\n",
        "\n",
        "            bucket_dir = f'blend.{yyyymmdd}/{req[\"hh\"]:02d}/{nbm_set}/'\n",
        "\n",
        "            grib_file = f'{bucket_dir}blend.t{req[\"hh\"]:02d}z.'+\\\n",
        "                        f'{nbm_set}.f{req[\"fhr\"]:03d}.{req[\"nbm_area\"]}.grib2'\n",
        "\n",
        "            index_file = f'{grib_file}.idx'\n",
        "\n",
        "            client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "            index_data_raw = client.get_object(\n",
        "                Bucket=aws_bucket, Key=index_file)['Body'].read().decode().split('\\n')\n",
        "\n",
        "            cols = ['num', 'byte', 'date', 'var', 'level',\n",
        "                'forecast', 'fthresh', 'ftype', '']\n",
        "\n",
        "            index_data = pd.DataFrame([item.split(':') for item in index_data_raw],\n",
        "                            columns=cols if nbm_set == 'core' else cols[:-1])\n",
        "\n",
        "            # Clean up any ghost indicies, set the index\n",
        "            index_data = index_data[index_data['num'] != '']\n",
        "            index_data['num'] = index_data['num'].astype(int)\n",
        "            index_data = index_data.set_index('num')\n",
        "\n",
        "            # Allow byte ranging to '' (EOF)\n",
        "            index_data.loc[index_data.shape[0]+1] = ['']*index_data.shape[1]\n",
        "\n",
        "            index_subset = index_data[\n",
        "                ((index_data['var'] == req['var']) &\n",
        "                (index_data['level'] == req['level']))]\n",
        "\n",
        "            # byte start >> byte range\n",
        "            for i in index_subset.index:\n",
        "                index_subset.loc[i]['byte'] = (\n",
        "                    index_data.loc[i, 'byte'],\n",
        "                    index_data.loc[int(i)+1, 'byte'])\n",
        "\n",
        "            # Filter out excluded vars\n",
        "            for ex in excludes:\n",
        "                mask = np.column_stack([index_subset[col].str.contains(ex, na=False)\n",
        "                                        for col in index_subset])\n",
        "\n",
        "                index_subset = index_subset.loc[~mask.any(axis=1)]\n",
        "\n",
        "            # Fetch the data by byte range, write from stream\n",
        "            for index, item in index_subset.iterrows():\n",
        "                byte_range = f\"bytes={item['byte'][0]}-{item['byte'][1]}\"\n",
        "\n",
        "                output_bytes = client.get_object(\n",
        "                    Bucket=aws_bucket, Key=grib_file, Range=byte_range)\n",
        "\n",
        "                with open(output_file, 'ab') as wfp:\n",
        "                    for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                        wfp.write(chunk)\n",
        "\n",
        "    client.close()\n",
        "    return output_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Input/Multiprocessing Inputs"
      ],
      "metadata": {
        "id": "AuVwvcXWhMPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "element = 'qpf' #input('Desired element? (QPF/MaxT/MinT)').lower()\n",
        "\n",
        "start_date = '20231001'\n",
        "end_date = '20231031'\n",
        "\n",
        "# Immediately convert user input to datetime objects\n",
        "start_date, end_date = [datetime.strptime(date+'0000', '%Y%m%d%H%M')\n",
        "    for date in [start_date, end_date]]"
      ],
      "metadata": {
        "id": "vtgrpivgQTNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main/Multiprocessing Call"
      ],
      "metadata": {
        "id": "Irb9HzFAhTZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build arg dict\n",
        "nbm_request_args = {\n",
        "    #'yyyymmdd':yyyymmdd, #input('Desired init date (YYYYMMDD)? '),\n",
        "    'hh':00, #int(input('Desired init hour int(HH)? ')),\n",
        "    'fhr':24, #int(input('Desired forecast hour/lead time int(HHH)?')),\n",
        "    'nbm_area':'co',\n",
        "    'var':element_var[element],\n",
        "    'level':element_lev[element]}"
      ],
      "metadata": {
        "id": "igAjCkwNhYYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= end_date:\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_grib_from_AWS, **nbm_request_args)\n",
        "\n",
        "# Set up this way for later additions (e.g. a 2D iterable)\n",
        "# multiprocess_iterable = [item for item in itertools.product(\n",
        "#     other_iterable, date_selection_iterable)]\n",
        "multiprocess_iterable = date_selection_iterable\n",
        "\n",
        "with Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} tasks '\n",
        "          f'across {process_pool_size} workers')\n",
        "    output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "    print('Multiprocessing Complete')"
      ],
      "metadata": {
        "id": "1-ReDYLChqO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}